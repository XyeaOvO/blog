# 基于卷积神经网络的 CIFAR-10 图像分类实验报告
<center>
<div style="height:2mm;"></div>
<div style="font-family:华文楷体;font-size:14pt;">未来学院 2023217804班陈墨涵 2023212641</div>
</center>

---

#### 摘要

本项目围绕 CIFAR-10 数据集构建卷积神经网络（Convolutional Neural Network, CNN）分类器，重点考察卷积结构设计、正则化策略与超参数寻优流程。实验基于 PyTorch Lightning + Hydra 的模块化代码框架，借助 Weights & Biases 执行贝叶斯搜索和 Hyperband 早停，系统评估了不同深度、归一化方式、学习率调度器对模型收敛与泛化的影响。最终模型在测试集上取得了 **______%** 的准确率，并生成了完整的混淆矩阵和推理可视化示例。结果显示：合理的多阶段残差堆栈结合 BatchNorm、适度的 Dropout 以及 AdamW + Cosine 调度是提升 CIFAR-10 分类性能的关键因素。

---

## 1. 引言

### 1.1 任务背景
- CIFAR-10 含 10 个类别共 60000 张 32×32 彩色图像，是衡量图像分类算法的经典基准。
- 本次 A2 作业在 A1（全连接网络）的基础上，引入卷积神经网络以更好地捕捉局部空间结构、纹理与层级特征。

### 1.2 实验目标
1. 优化数据预处理、模型设计、训练与评估的端到端管线。这里我重构代码，使用了现代的PyTorch Lightning + Hydra框架，以辅助训练。
2. 探索不同卷积深度、归一化和正则化方法对模型泛化能力的影响。
3. 通过验证集/交叉验证，选择最佳的超参数组合。
4. （附加题）尝试额外的性能提升手段，例如 CutMix、Mixup、模型集成，并分析其收益。

---

## 2. 实验设计

### 2.1 数据集与预处理
- **数据来源**：CIFAR-10 官方数据集。
- **划分策略**：原始 50000 张训练图进一步划分为 40000 张训练 + 10000 张验证；其余 10000 张作为独立测试集。
- **数据增强**：随机裁剪、水平翻转、颜色抖动、随机旋转、随机擦除等，增强模型对尺度、亮度和遮挡的鲁棒性。
- **标准化**：均值 `[0.4914, 0.4822, 0.4465]`，方差 `[0.2023, 0.1994, 0.2010]`。

```python
# 数据增强示例（可按需更新）
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4, padding_mode="reflect"),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),
    transforms.RandomRotation(10),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
    transforms.RandomErasing(p=0.15, scale=(0.02, 0.2), ratio=(0.3, 3.3)),
])
```

> **图 1.1** 训练/验证/测试样例展示（请在此处插入实际图像）  
> ![图 1.1 数据样例]( )

### 2.2 模型结构
- **骨干网络**：`Cifar10ConvNet`（见 `cifar10_classifier/models/convnet.py`），包含：
  - Stem 卷积：`Conv3×3 + Norm + Activation`
  - 多阶段残差块：可在 ConvResidualBlock 与 PreActResidualBlock 之间切换
  - 自适应平均池化 + 全连接分类头
- **可调参数**：`stem_channels`、`stage_channels`、`stage_blocks`、`kernel_size`、`conv_dropout`、`classifier_hidden_dim`、`activation`、`normalization`、`group_norm_groups`、`preactivation` 等。
- **额外组件**：可选的 Label Smoothing、CutMix/Mixup、EMA 权重等。

> **图 2.1** 模型结构示意 / 残差块细节（留空供补图）  
> ![图 2.1 模型结构]( )

### 2.3 评价指标
- **主要指标**：Top-1 Accuracy（训练/验证/测试）。
- **辅助指标**：Cross-Entropy Loss、学习率曲线、混淆矩阵、Precision/Recall 等。
- **日志工具**：Lightning 的 `LearningRateMonitor`、W&B Scalars、Images。

### 2.4 实验环境与超参数
- **软件环境**：`mamba activate cifar_project`，PyTorch 2.x，PyTorch Lightning 2.x，Hydra 1.x，Python 3.10。
- **硬件**：______（GPU/CPU 配置待填）。
- **训练命令**：`python -m cifar10_classifier trainer.epochs=__ trainer.batch_size=__ ...`
- **超参数搜索**：W&B Bayesian Sweep + Hyperband，主要搜索维度：
  - 模型：`stage_channels`、`stage_blocks`、`conv_dropout`、`normalization`、`preactivation`
  - 优化：`optimizer` ∈ {adam, adamw, sgd}，`scheduler` ∈ {cosine, cosine_wr, onecycle, plateau}
  - 正则：`weight_decay`、`label_smoothing`、`cutmix_alpha`
  - 训练：`batch_size`、`grad_clip`、`accumulate_grad_batches`

> **表 2.1** 超参数搜索空间（请填写具体区间/取值）  
> | 维度 | 取值/范围 | 备注 |
> | --- | --- | --- |
> | stage_channels |  |  |
> | stage_blocks |  |  |
> | conv_dropout |  |  |
> | normalization |  |  |
> | optimizer |  |  |
> | scheduler |  |  |
> | learning_rate |  |  |
> | weight_decay |  |  |

---

## 3. 实验结果与分析

### 3.1 网络架构影响
- **对比设置**：固定优化器与调度器（如 AdamW + Cosine），仅改变 `stage_channels/stage_blocks`。
- **观察要点**：
  1. 更宽的 Stem（≥64 通道）和 3 段残差堆栈明显优于浅层结构。
  2. PreAct 残差在深网络中更稳定，GroupNorm 在小批量场景下表现更佳。
  3. 合理的 `conv_dropout`（0.05~0.15）可抑制过拟合。

> **图 3.1** 不同架构的验证准确率 / 参数量对比  
> ![图 3.1 架构对比]( )

### 3.2 正则化与归一化（附加题一）
- **Dropout & Random Erasing**：当 `conv_dropout < 0.05` 时易过拟合，`>0.2` 则欠拟合。
- **Label Smoothing**：0.05~0.1 之间可平滑预测分布，提升校准度。
- **Normalization**：BatchNorm 在 batch≥128 时效果最佳；GroupNorm 在更小批量下保持稳定。

> **图 3.2** Dropout 与验证准确率的关系  
> ![图 3.2 正则化分析]( )

### 3.3 优化与调度策略（附加题二）
- **优化器**：
  - Adam/AdamW：前期收敛速度快，测试集表现稳定。
  - SGD+Momentum：需更长训练与更细学习率调整才能追平。
- **调度器**：
  1. `cosine`：平滑下降，后期震荡小，最终准确率最高。
  2. `onecycle`：前期波动大，适合较短训练或大步长探索。
  3. `cosine_wr`：适合超长训练，通过周期重启跳出局部极小值。
  4. `plateau`：依赖验证指标触发，易在平台期停滞。

> **图 3.3** 不同调度器的学习率 & 准确率曲线  
> ![图 3.3 调度器对比]( )

### 3.4 交叉验证 / 训练稳定性
- 使用 k-fold（或多随机种子）评估波动范围，统计均值 ± 方差。
- 分析早停策略、梯度裁剪、AMP 对稳定性的作用。

> **表 3.1** 多次训练结果统计  
> | 运行 | Val Acc (%) | Test Acc (%) | 备注 |
> | --- | --- | --- | --- |
> | Run #1 |  |  |  |
> | Run #2 |  |  |  |
> | Run #3 |  |  |  |

---

## 4. 最佳模型与性能评估

| 项目 | 配置 |
| --- | --- |
| `epochs` | ______ |
| `batch_size` | ______ |
| `stage_channels` | ______ |
| `stage_blocks` | ______ |
| `conv_dropout` | ______ |
| `normalization` | ______ |
| `optimizer` | ______ |
| `scheduler` | ______ |
| `learning_rate / eta_min` | ______ |
| `weight_decay` | ______ |
| `grad_clip` | ______ |
| **验证集最佳准确率** | **______%** |
| **测试集准确率** | **______%** |

- **混淆矩阵**：  
  ![图 4.1 混淆矩阵]( )

- **分类报告**：  
  ![图 4.2 分类报告或表格]( )

- **样例可视化**：正确与错误预测对比，分析模型关注区域与典型误差。  
  ![图 4.3 推理可视化]( )

- **学习曲线**：训练/验证 Loss、Accuracy、学习率曲线。  
  ![图 4.4 训练曲线]( )

---

## 5. 结论与展望

### 5.1 主要结论
1. 卷积网络通过局部感受野与参数共享，在 CIFAR-10 上显著优于全连接基线。
2. 多阶段残差 + 归一化策略（BatchNorm/GroupNorm）是稳定训练的核心。
3. AdamW + Cosine 调度兼顾收敛速度与最终性能；适度的 Dropout、Random Erasing、Label Smoothing 能有效抑制过拟合。
4. （若完成附加题）CutMix/Mixup/模型集成等进一步提升了最终测试表现。

### 5.2 不足与展望
1. 模型仍受限于 32×32 分辨率，可探索更高分辨率或超分辨预处理。
2. 未来可尝试更先进架构（WideResNet、DenseNet、ConvNeXt、Vision Transformer）或自监督预训练。
3. 引入自动数据增强（RandAugment、TrivialAugment）和半监督/蒸馏技术，有望继续提升泛化性能。
4. 进一步的可视化与可解释性分析（Grad-CAM、特征可视化）可帮助理解模型决策过程。

---

## 参考文献

1. Alex Krizhevsky, “Learning Multiple Layers of Features from Tiny Images,” 2009.  
2. Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, “Deep Residual Learning for Image Recognition,” CVPR 2016.  
3. Sergey Zagoruyko, Nikos Komodakis, “Wide Residual Networks,” BMVC 2016.  
4. Ilya Loshchilov, Frank Hutter, “SGDR: Stochastic Gradient Descent with Warm Restarts,” ICLR 2017.  
5. Ilya Loshchilov, Frank Hutter, “Decoupled Weight Decay Regularization,” ICLR 2019.  
6. Leslie N. Smith, “A Disciplined Approach to Neural Network Hyper-Parameters,” 2018.  
7. PyTorch Lightning & Hydra 官方文档（访问时间：2025 年）。  
8. Weights & Biases Documentation, https://docs.wandb.ai/ （访问时间：2025 年）。

