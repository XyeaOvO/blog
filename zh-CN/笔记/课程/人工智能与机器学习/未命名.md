# **基于全连接神经网络的CIFAR**-10图像分类实验报告
<center> <div style='height:2mm;'> </div> <div style="font-family:华文楷体;font-size:14pt;">未来学院 2023217804班陈墨涵 2023212641</div> </center>

---

#### **摘要**

本项目旨在设计并实现一个用于CIFAR-10图像分类的全连接神经网络。我系统性地探索了不同网络架构、损失函数、正则化方法以及优化算法对模型性能的影响。通过使用 Weights & Biases 工具进行超参数的贝叶斯搜索，我确定了一组最优配置。最终，我的最佳模型在CIFAR-10测试集上达到了 [你的最终准确率]% 的准确率。实验结果表明，[此处填写你最重要的1-2个发现，例如：合适的网络深度和正则化策略是防止过拟合的关键，而AdamW优化器与Cosine退火学习率调度器的组合能够实现快速且稳定的收敛]。

---

### **1. 引言**

#### **1.1 任务背景**
图像分类是计算机视觉领域的一项基础且核心的任务。本次作业要求我使用经典的全连接神经网络（Fully Connected Neural Network, FCN）对CIFAR-10数据集进行分类。CIFAR-10数据集包含了10个类别的60000张32x32的彩色图像，是一个广泛用于评测图像分类算法性能的基准数据集。

#### **1.2 实验目标**
本实验的主要目标包括：
*   理解并搭建一个完整的图像分类流程，包括数据预处理、模型设计、训练与评估。
*   掌握通过验证集调整超参数的方法，理解训练集、验证集和测试集的划分意义。
*   （附加任务）探究不同损失函数（如交叉熵、Focal Loss）和正则化方法（如Dropout、权重衰减）对模型性能及泛化能力的影响。
*   （附加任务）对比分析不同优化算法（如SGD, AdamW）及学习率调度策略（如Plateau, Cosine）对模型训练过程和最终性能的影响。

---

### **2. 实验设计**

#### **2.1 数据集**
*   **数据集**: CIFAR-10
*   **图像尺寸**: 3x32x32
*   **类别数量**: 10
*   **数据划分**: 训练集50000张，测试集10000张。在训练过程中，我将原始训练集进一步划分为训练集（40000张）和验证集（10000张），用于模型选择和超参数调整。
*   **数据增强**: 我对数据集采用了多种数据增强方法，以增强模型的泛化性，防止过拟合，具体设置如下：
	```python
      transform_train = transforms.Compose(
        [
            transforms.RandomCrop(32, padding=4, padding_mode="reflect"),# 随机裁剪
            transforms.RandomHorizontalFlip(),# 随机水平翻转
            transforms.RandomApply(
                [
                    transforms.ColorJitter(
                        brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1
                    )# 调整图像的亮度、对比度、饱和度和色调
                ],
                p=0.3,
            ),# 以一定概率 p 应用一组变换
            transforms.RandomRotation(10),# 随机旋转
            transforms.ToTensor(),
            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
            transforms.RandomErasing(p=0.15, scale=(0.02, 0.2), ratio=(0.3, 3.3)),
        ]# 随机擦除
    )
	```

#### **2.2 模型结构**
我使用的模型是一个MLP。其基本结构如下：

`Input(3072) -> [Linear -> BatchNorm -> Activation -> Dropout] -> ... (重复N次) -> Linear(10) -> Softmax`

其中，隐藏层的数量（深度）、每层的神经元数量（宽度）、激活函数、Dropout率和是否使用批量归一化（BatchNorm）都是我实验中探索的超参数。

#### **2.3 评价指标**
*   **主要指标**: **Accuracy**。衡量模型在给定数据集上正确分类的样本比例，根本任务。
*   **辅助指标**: **Loss**。判断模型是否有效收敛以及是否存在过拟合。

#### **2.4 实验环境与超参数**
*   **硬件**: 4 x NVIDIA RTX 3090
*   这里我使用Weights & Biases的Bayes Sweep进行日志记录和自动化超参数搜索实验。主要探索的超参数空间如下：
    *   **`hidden_sizes`**: `[1024,512,256]`, `[512,256,128]`, `[1024,512]`, `[1024,1024,512]`, `[512,1024,512]`, `[1024,1024]`, `[2048,512]`
    *   **`loss_function`**: `cross_entropy`, `focal_loss`, `label_smoothing`
    *   **`optimizer`**: `adamw`, `sgd`
    *   **`scheduler`**: `plateau`, `cosine`, `onecycle`
    *   **`learning_rate`**: `log_uniform(1e-4, 1e-3)`
    *   **`dropout`**: `uniform(0.2, 0.5)`
    *   **`weight_decay`**: `log_uniform(1e-5, 5e-4)`
*   **固定参数**:
    *   **`epochs`**: 300
    *   **`batch_size`**: 256

---

### **3. 实验结果与分析**
值得一提的是，在调参过程中，我发现深度学习模型的超参数其实是相互影响、相互依赖的，而不是孤立的。

比如说，一个对于 AdamW 优化器来说很完美的学习率，如果直接用在 SGD 优化器上，效果可能会非常差。

所以，虽然下面的章节为了清晰起见，会把网络架构、损失函数与正则化、优化算法等因素分开来独立讨论，但我们最终的目标是找到这些参数的最佳组合，而不是单个的万金油最佳值。这也正是我们采用 wandb 的Bayes Sweep来进行自动化实验的原因——它能够帮助我们在这样一个复杂的多维空间中，更智能地寻找全局最优的超参数配置。

同时，为了更有效率的探索，我开启了hyperband早停策略，并不是每个超参组合都会运行到最后。

#### **3.1 网络架构的影响**
**过滤器设置：** scheduler=cosine、optimizer IN adam or adamw
![[Pic/Pasted image 20251102113730.png]]

---

#### **3.2 损失函数与正则化方法分析 (附加题1)**
**过滤器设置：** scheduler=cosine、optimizer IN adam or adamw
**实验目的**: 对比不同损失函数的效果，并分析正则化策略对抑制过拟合的作用。

**3.2.1 损失函数对比**
![[Pic/Pasted image 20251102114047.png]]

**结果分析**:
label_smoothing前期好于cross_entropy，后期效果与cross_entropy相近，两者均明显好于focal_loss。

**3.2.2 正则化效果分析**

![[Pic/Pasted image 20251102114319.png]]

**结果分析**:
当我筛选Val/Accuracy≥60%的结果时，有一个很明显的现象就是dropout率集中在[0.3,0.45]区间内。而这些结果的`weight_decay`在`(1e-5, 5e-4)`范围内分布较为均匀，没有显著偏好。

---

#### **3.3 优化算法分析 (附加题2)**

**实验目的**: 比较不同优化器和学习率调度器对模型收敛速度和最终性能的影响。
**optimizer的对比：**
![[Pic/Pasted image 20251102115313.png]]
**分析：**

**scheduler的对比：**
![[Pic/Pasted image 20251102115403.png]]

**结果分析**:
从图3.4可以看出，`AdamW`优化器的收敛速度远快于`SGD`。`AdamW`在约 [填写数值] 个epoch后就达到了较高的准确率，而`SGD`收敛较慢。这体现了自适应学习率优化算法的优势。
从图3.5可以看出，`Cosine`和`OneCycle`调度器表现优于`Plateau`。`OneCycle`策略在训练初期使用一个较高的学习率帮助模型跳出局部最优，然后逐渐下降，其准确率曲线上升最快。`Cosine`调度器平滑地降低学习率，最终也达到了与之相当的性能，且在训练后期更为稳定。`Plateau`调度器由于[填写原因，例如：在验证集性能停滞前保持学习率不变，导致前期收敛速度不如前两者]。

---

### **4. 最佳模型与性能评估**

#### **4.1 最佳模型配置**
通过对所有实验运行的分析，我从W&B中筛选出在验证集上准确率最高的模型。其超参数配置如下：

| 超参数           | 最佳值                          |
| :--------------- | :------------------------------ |
| `hidden_sizes`   | `1024, 512, 256`                |
| `loss_function`  | `label_smoothing`               |
| `label_smoothing`| `0.1` (示例值)                  |
| `optimizer`      | `adamw`                         |
| `scheduler`      | `cosine`                        |
| `learning_rate`  | [从W&B中找到最佳值]             |
| `dropout`        | [从W&B中找到最佳值]             |
| `weight_decay`   | [从W&B中找到最佳值]             |
| **验证集最高准确率** | **[填写具体数值]%**           |

#### **4.2 最终性能评估**
我使用上述最佳超参数配置，重新训练了模型，并在**从未用于训练和调优的测试集**上进行了最终评估。

*   **测试集准确率**: **[填写最终测试准确率]%**
*   **测试集损失**: **[填写最终测试损失]**

**(图4.1：最佳模型在测试集上的混淆矩阵)**
> **[代码生成并粘贴在此处]**
> *   **操作指南**: 编写一小段代码，用你最终的模型对测试集进行预测，然后使用`sklearn.metrics.confusion_matrix`和`seaborn.heatmap`来绘制混淆矩阵。

**结果分析**:
混淆矩阵直观地展示了模型在各个类别上的性能。从图中可以看出，模型在 [例如：'飞机', '汽车'] 等类别上表现很好，但在 [例如：'猫'和'狗'] 之间存在一些混淆。这可能是因为[分析原因，例如：这两类在视觉上特征相似，而全连接网络无法有效学习空间特征]。

---

### **5. 结论与展望**

#### **5.1 结论总结**
本次实验成功地构建了一个用于CIFAR-10分类的全连接神经网络。我通过系统的超参数调优，发现：
1.  一个三层、宽度递减的较深网络（`1024,512,256`）是处理此任务的有效架构。
2.  正则化（特别是Dropout和标签平滑）对于防止过拟合、提升模型泛化能力至关重要。
3.  `AdamW`优化器与`Cosine`学习率调度器的组合，在收敛速度和最终性能上都表现出色。

我的最佳模型最终在测试集上取得了 **[你的最终准确率]%** 的准确率，完成了本次作业的所有基本和附加任务。

#### **5.2 思考与展望 (额外加分项)**
尽管全连接网络能够完成分类任务，但其性能存在瓶颈，主要因为它忽略了图像的空间结构信息。未来的改进方向可以包括：
1.  **使用卷积神经网络（CNN）**: CNN专门为处理网格状数据（如图像）而设计，其卷积核能够有效提取局部空间特征，在图像分类任务上通常远胜于FCN。
2.  **数据增强（Data Augmentation）**: 在训练过程中对图像进行随机旋转、裁剪、翻转等操作，可以扩充数据集，让模型学习到更具不变性的特征，从而提高泛化能力。
3.  **更先进的架构**: 尝试引入残差连接（ResNet）等更现代的神经网络结构，以训练更深、性能更强的模型。

---
### **附录**

#### **A. 核心代码说明**
*   **`train.py`**: 主训练脚本，负责解析命令行参数、初始化模型、优化器、数据加载器，并执行训练和验证循环。
*   **`config.py`**: 定义了`Config`类和`argparse`解析器，用于管理所有超参数。
*   **`model.py`**: 定义了全连接神经网络的模型结构，能够根据配置动态生成不同深度和宽度的网络。
*   **`loss.py`**: 损失函数工厂，可以根据名称构建不同的损失函数，如Focal Loss。

#### **B. `wandb_sweep.yaml` 配置文件**
```yaml
# 在此粘贴你最终使用的 sweep_enhanced.yaml 文件内容
...
```