### 1. 项目目标

我的研究目标是训练一个**下一最佳视角（Next-Best-View, NBV）策略网络 $\pi_{\theta}$**。该网络能够主动指导一个通用的三维重建模型 `MapAnything`，通过迭代式地预测信息最丰富的下一个相机位姿，从而以最少的视图数量，最大化最终三维重建的质量。

### 2. 核心组件

我的研究框架主要由以下三个部分构成：

- **三维重建骨干 $\mathcal{M}$ (`MapAnything`)**: 我采用现有的 `MapAnything` 作为一个强大、统一且前馈式的三维重建骨干。它的核心是一个为多视图几何设计的**24层交替注意力Transformer**。
    
    - **输入与预处理**: 模型首先使用 DINOv2 将输入的 $N$ 张图像编码为图像块令牌（Patch Tokens）。同时，它能够灵活地融合为任意视图子集提供的可选几何信息，如相机位姿、深度图、内参。一个特殊的可学习**尺度令牌**会被附加到这 $N$ 组视图令牌之后，同时，一个固定的**参考视图嵌入**会被加到第一个视图的令牌上，以区分参考坐标系。
        
    - **核心融合机制**: Transformer 通过其交替注意力的结构，在视图内（Intra-View）和视图间（Inter-View）两个维度上高效地融合所有 $N$ 个视图的信息。
        
    - **分解式输出**: Transformer 的输出令牌会被送入不同的解码头，分别预测一个全局**度量尺度因子** ($m$)，以及每个视图局部的**光线方向** ($R_i$)、**up-to-scale 的光线深度** ($\tilde{D}_i$) 和**相机位姿** ($\tilde{P}_i$)。最终的度量点云是通过将这些分解式的输出组合计算而成的。
        
- **NBV 策略网络 $\pi_{\theta}$**: 这是我需要训练的核心模块。它的输入，即场景特征 $F_N$，正是从 `MapAnything` 的多视图Transformer 的最后一层的输出令牌。该特征张量的形状为 **$[B, N, P=(H/14)*(W/14), 768]$**，其中 $B$ 是批次大小， $N$ 是初始视图数，$(H/14)*(W/14)$ 是每个视图的图像块数量，具体来说是518/14\*518/14=1369，而 $768$ 则是 Transformer 的隐层维度。我的网络接收这个高维特征 $F_N$ ，在$P$维度上平均池化后，回归出下一个最佳相机位姿（相对第一个View的相对位姿） $P_{N+1}$（目前我只预测三维坐标，旋转自动朝向物体）。
    
- **可微分渲染器 $\mathcal{R}$**: 我基于 PyTorch3D 构建了一个可微分渲染器，用于根据给定的真值三维网格 $M_{GT}$ 和一个相机位姿 $P_{N+1}$，合成一个新的观测视图 $V_{N+1}$（RGB图、深度图、点云图）。
    
### 3. 训练流程

我采用端到端监督学习的方式来训练我的策略网络。

我目前的训练流程如下：

1. **状态表征**: 给定 $N$ 个初始视图（$V_i$是RGB图、深度图和内参） $\{V_i, P_i\}_{i=1}^N$，我将它们送入 `MapAnything` 提取场景特征 $F_N$。
    
2. **策略预测:** 我的策略网络 $\pi_{\theta}$ 接收 $F_N$ 并回归出下一个相机位姿：
    
    $P_{N+1} = \pi_{\theta}(F_N)$
    
3. **环境交互:** 使用 $\mathcal{R}$ 合成新视图：$V_{N+1} = \mathcal{R}(M_{GT}, P_{N+1})$
    
4. **重建与评估**: 从完整的视图集 $\{V_1, \dots, V_N, V_{N+1}\}$ 中可以直接得到预测点云 $\mathcal{P}_{pred}$。
    
5. **损失函数:** 使用**推土机距离（Earth Mover's Distance, EMD）** 作为最终的损失函数，具体参考附件。

6. **反向传播**