# 1. What's the Mapanything?
一个统一 (Unified) 的三维重建框架，用单一模型解决三维重建任务：
![[Pic/Pasted image 20251010132316.png]]
Motivation：
*   传统方法：将三维重建分解为多个独立的、串联的步骤（特征匹配 -> 位姿估计 -> BA优化 -> MVS稠密重建），流程复杂、耗时，且错误会累积。
*   近期深度学习方法：虽然出现了前馈式模型，但它们通常是“专才”，即一个模型只解决一个特定任务（如仅MVS或仅相机定位），且输入格式固定，不够灵活。
*   缺乏一个既能处理各种输入组合，又能一步到位（前馈式）输出带真实尺度结果的通用 (Universal) 三维重建模型。
# 2. What's New in MapAnything?
为了实现这种通用性，

*   分解式场景表示 (Factored Scene Representation):
    *   模型将一个复杂的三维场景解耦 (decouple) 为四个更容易预测的部分：
        1.  光线方向: 逐像素的光线方向，代表相机内参。
        2.  相对深度: 逐像素的、无真实尺度的深度，代表场景的几何形状。
        3.  相对位姿: 相机相对于参考视图的位姿，代表相机的空间位置。
        4.  单一的尺度因子: 一个全局的标量值，用于将整个相对尺度的重建结果缩放到真实物理尺度。
- 专门设计一个尺度令牌
## 3. Methodology
![[Pic/Pasted image 20251010132333.png]]
- 如何编码、融合特征？
	- 图像: 将每张输入图像 $\hat{I}_i$ 用DINOv2 (ViT-L)编码为 $H/14 \times W/14$ 个区块特征(patch features) $F_{\hat{I}_i}$，每个特征维度为1024。
	- 几何输入编码：
		-  预处理: 在编码前，先对几何量进行 **“尺度-形状”分解**，以实现对有/无度量尺度数据的统一处理。
	        *   **深度 $\tilde{D}_i$**: 分解为逐视图的平均深度 $\hat{d}_i$ (尺度) 和归一化的深度图 $\tilde{D}_i / \hat{d}_i$ (形状)。
	        *   **平移 $\hat{T}_i$**: 分解为所有视图的平均位姿尺度 $\hat{s}_p$ (尺度) 和归一化的平移方向 $\hat{T}_i / \hat{s}_p$ (形状)。
	        *   尺度信息($\hat{d}_i, \hat{s}_p$)仅在输入数据明确为“可度量”时才作为特征使用，并会先进行**对数变换**。
	    * 编码：
		    *   稠密几何编码 (光线 $R_i$, 归一化深度)**:
		        *   **方法**: 使用一个浅层卷积编码器，通过一次 `size=14` 的**像素重组 (Pixel Unshuffle)** 操作，高效地将空间分辨率对齐到 DINOv2 的 $H/14 \times W/14$。
		    *   全局几何编码 (旋转 $Q_i$, 归一化平移, 尺度)**:
		        *   **方法**: 使用一个 **4 层的 MLP** 将这些非像素级的向量/标量映射到1024维的特征空间。
	- 融合：对特征先进行**层归一化 (Layer Normalization)**，然后**逐元素相加 (Summation)**，最后再进行一次层归一化
	- 融合后的特征被展平，并额外附加一个可学习的尺度令牌，同时为视图1的令牌加上一个参考视图嵌入，共同构成送入 Transformer 的最终输入序列。
- 如何解码特征？
	- 位姿解码 (旋转, 平移):
	    *   使用一个基于平均池化的卷积位姿头。
- 如何训练？
	*   概率性输入: 在训练的每个步骤中，是否提供几何信息、提供哪几种、提供给哪些视图、深度信息是稠密还是稀疏、是否告知模型尺度等，都由一系列**预设的概率**来随机决定。
	*   多视图采样: 采用**基于共视性**的随机游走采样策略，确保每个训练批次(batch)中的 N 张视图在空间上是连贯的、有意义的。
- 如何设计loss?
	*   **主要几何损失**: $\mathcal{L}_{\text{pointmap}}$ (置信度加权的全局点云损失), $\mathcal{L}_{\text{lpm}}$ (局部点云损失), $\mathcal{L}_{\text{depth}}$, $\mathcal{L}_{\text{translation}}$。
    *   **细节增强损失**: $\mathcal{L}_{\text{normal}}$ (法线损失), $\mathcal{L}_{\text{GM}}$ (多尺度梯度匹配损失)，仅在合成数据上使用。
    *   **辅助损失**: $\mathcal{L}_{\text{mask}}$ (二元交叉熵损失)。
    *   **尺度损失**: $\mathcal{L}_{\text{scale}}$。
	    * ![[Pic/Pasted image 20251010191822.png]]
	    * ![[Pic/Pasted image 20251010191752.png]]
	- ![[Pic/Pasted image 20251010191433.png]]
## 4. Experimental Results 
![[Pic/Pasted image 20251010185756.png]]
# 5. Bring Home Message
- 最直接的：让位姿预测支持各种各样的输入（目前是img+pose）
- 输入设计：在我们的 NBV 框架中，动作预测网络的输入可以：一部分是来自大感知模型的、高度抽象的**场景几何状态表征**，另一部分是**显式的相机位姿序列**。
- 真实数据相机位姿没法归一化，将真实的相机位姿先除以 L_metric 进行归一化，过nbv网络后可以反归一化。
- 旋转到首相机参考坐标系下进行对比
- 场景归一化、pose头设计、数据集可以参考