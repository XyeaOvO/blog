### 3. 具身世界模型优化 (30 分)

具身智能（Embodied Intelligence）代表了人工智能领域的一个变革性前沿，它在统一的系统中集成了感知、推理和物理动作。通过使智能体能够直接与物理世界交互，具身智能弥合了数字计算与现实世界行动力之间的鸿沟。通用人形机器人在该领域处于核心地位，因为它们必须在非结构化环境中有效运行，操纵各种物体，并展示出对物理原理的实际理解。

为了让人形机器人有目的地行动，有效的运行依赖于决策制定与环境模拟之间的紧密耦合。**视觉-语言-动作 (VLA)** 模型作为智能体的认知核心，负责将语言指令和感知输入转化为具体的动作建议。与之互补的是**学习型世界模型（Learned World Model）**，它充当环境的神经模拟器。通过评估 VLA 生成的候选动作，世界模型通过生成物理上写实的视频序列来预测未来结果。这种形式的“精神排演（mental rehearsal）”能够创建物理上合理的现实世界数字孪生，从而支持策略评估和安全性验证，而无需承担物理试错的风险。然而，与 VLA 的离散动作输出相比，世界模型必须合成高维、连续的视频表示，这使其成为整个系统中的主要计算瓶颈。

为了解决这一计算瓶颈并实现实际部署，本次挑战赛的预赛重点在于加速 **UnifoLM-WMA-0**，这是由 **宇树科技 (Unitree Robotics)** 开发的用于具身智能任务的最先进世界模型。参赛者被要求在指定的场景和命令输入下优化模型的推理性能，从而实现机器人的快速“精神排演”，同时必须严格保持定义的视觉质量阈值。

该任务将使用组委会提供的 5 个机器人场景进行评估。每个场景中选出 4 个代表性样本，共计 20 个评估样本。**每个样本测得的推理时间将作为主要的定量性能指标。**

**峰值信噪比 (PSNR)** 将用于评估参赛者生成的视频与组委会提供的参考视频之间的视觉保真度。对于 20 个评估样本中的每一个，生成的视频必须达到 **PSNR ≥ 25** 的要求。如果给定场景中的任何一个样本未能满足 PSNR ≥ 25 的要求，则该场景将获得零分。

#### 工作流程教程 (Workflow Tutorial)

**(1) 安装 UnifoLM-WMA-0 项目并下载模型权重 (Checkpoints)**

*   UnifoLM-WMA-0 可以通过 GitHub 仓库访问：`https://github.com/unitreerobotics/unifolm-world-model-action`
*   UnifoLM-WMA-0 的模型权重可以从以下地址获取：`https://huggingface.co/unitreerobotics/UnifoLM-WMA-0-Dual`

**(2) 下载输入文件**

从 ASC 官方仓库 `https://github.com/ASC-Competition` 下载 5 个场景（共 20 个样本）的初始条件和参考视频，并将它们放入 UnifoLM-WMA-0 仓库中。以下是单个场景的输入文件结构示例：

```text
UnifoLM-WMA-0/unitree_g1_pack_camera
├── case1
│   ├── run_world_model_interaction.sh
│   ├── unitree_g1_pack_camera_case1.mp4
│   └── world_model_interaction_prompts
│       ├── images
│       │   └── unitree_g1_pack_camera
│       │       └── 0.png
│       ├── transitions
│       │   └── unitree_g1_pack_camera
│       │       ├── 0.h5
│       │       └── meta_data
│       │           └── stats.safetensors
│       └── unitree_g1_pack_camera.csv
├── case2
├── case3
└── case4
```

**(3) 运行推理**

根据 `https://github.com/unitreerobotics/unifolm-world-model-action` 的 README 文件，在 `configs/inference/world_model_interaction.yaml` 中指定正确的 `data_dir` 绝对路径。

完成上述准备后，您可以使用以下命令执行推理任务：

```bash
cd unifolm-world-model-action
bash {scenario_name}/{case_id}/run_world_model_interaction.sh
# 例如: bash unitree_z1_dual_arm_stackbox_v2/case1/run_world_model_interaction.sh
```

执行后，脚本将产生类似于以下的输出：

```text
 0%| 
| 0/8 [00:00<?, ?it/s]>>> Step 0: generating actions ...
>>> Step 0: interacting with world model ...
>>>>>>>>>>>>>>>>>>>>>>>>
12%| █████████████████████████████████████ ▍ 
| 1/8 [00:46<05:22, 46.03s/it]>>> Step 1: generating actions ...
>>> Step 1: interacting with world model ...
>>>>>>>>>>>>>>>>>>>>>>>>
...
100%|█████████████████████████████████████
8/8 [06:10<00:00, 46.29s/it]

real    8m15.226s
user    278m50.952s
sys     17m21.528s
```

在本次挑战赛中，**real time（实际用时）被视为推理时间**。在上述 `unitree_g1_pack_camera` 数据集的 1 个样本示例中，推理时间为 `8m15.226s`。

推理过程完成后，将生成一个输出目录。以下是从 `unitree_g1_pack_camera` 数据集的一个案例推理中获得的目录结构示例：

```text
UnifoLM-WMA-0/unitree_g1_pack_camera
├── case1
│   ├── output
│   │   ├── inference
│   │   │   ├── 0_full_fs6.mp4
│   │   │   └── sample_0
│   │   │       ├── dm
│   │   │       │   └── 6
│   │   │       │       ├── itr-0.mp4
│   │   │       │       └── ... (itr-1 到 itr-9.mp4)
│   │   │       └── wm
│   │   │           └── 6
│   │   │               ├── itr-0.mp4
│   │   │               └── ... (itr-1 到 itr-9.mp4)
│   │   └── tensorboard
│   │       └── events.out.tfevents...
│   └── output.log
├── case2
├── case3
└── case4
```

**(4) PSNR 评估**

从 ASC 仓库 `https://github.com/ASC-Competition` 下载 PSNR 评估脚本和参考视频。然后，执行 PSNR 计算脚本以获得输出：

```bash
python3 psnr_score_for_challenge.py --gt_video path/to/gt/videos --pred_video path/to/pred/video --output_file path/to/output/json

# 示例:
python3 psnr_score_for_challenge.py \
--gt_video unitree_g1_pack_camera/case1/unitree_g1_pack_camera_case1.mp4 \
--pred_video unitree_g1_pack_camera/case1/unitree_g1_pack_camera_case1_fake.mp4 \
--output_file unitree_g1_pack_camera/case1/psnr_result.json
```

该脚本将生成一个包含单个样本 PSNR 分数词典的 JSON 文件。JSON 文件内容示例如下（本示例中的 PSNR 值仅用于说明，不得作为参考）：

```json
{
 "gt_video": "unitree_g1_pack_camera/case1/unitree_g1_pack_camera_case1.mp4",
 "pred_video": "unitree_g1_pack_camera/case1/output/inference/0_full_fs6.mp4",
 "psnr": 28.32714035028894
}
```

#### 结果提交 (Result Submission)

请将所有要求的文件压缩为一个名为 `Teamname_worldmodel.tar.gz` 的文件进行提交。目录结构如下：

```text
Teamname_worldmodel
├── code (源代码)
├── proposal_file (提案文件)
└── results (结果)
    ├── summary.json
    ├── unitree_g1_pack_camera
    │   ├── case1
    │   │   ├── 0_full_fs6.mp4
    │   │   └── output.log
    │   ├── case2...
    ├── unitree_z1_dual_arm_cleanup_pencils (其他场景目录)
    └── ...
```

**注意事项：**
1. 必须提供项目的**完整源代码**。
2. 必须提供 5 个场景共 20 个案例的目录。每个案例目录必须包含完整的 MP4 视频文件（如 `0_full_fs6.mp4`）以及包含推理过程中实时输出屏幕内容的 `output.log` 文件。
3. 必须提供一个 `summary.json` 文件。请严格按照指定格式生成该 JSON 文件。示例中显示的耗时和 PSNR 值仅供参考，不得作为评估依据。

`summary.json` 格式示例（需包含 20 个字典的列表）：
```json
[
 {"scenario": "unitree_g1_pack_camera",
  "case_id": 1,
  "time_before_optim": 606.984,
  "time_after_optim": 397.532,
  "psnr": 28.3627},
  ...
]
```

4. **提案文件 (Proposal file)**：应以 DOC 或 PDF 格式提交。需包含评估结果、优化方法的全面描述，以及所有评估样本优化前后的执行时间对比。

#### 评估标准概览 (Evaluation Criteria Overview)

在评估过程中，ASC26 组委会将**重点关注推理性能的提升**。此外，能够清晰陈述优化策略并结合底层技术原理解释的提案将是评分的关键基础。以下是参赛团队的重要注意事项：

1. **模型权重参数限制为最低 16 位精度**；不允许对权重进行低于 16 位的量化（sub-16-bit quantization）。
2. **不得修改**每个案例目录下的 `run_world_model_interaction.sh` 文件（`CUDA_VISIBLE_DEVICES` 除外）。**不得修改** `configs/inference/world_model_interaction.yaml` 文件（`data_dir` 除外）。
3. **不得修改**提供的输入文件。
4. 请保持目录结构和生成的结果文件名称不变。
5. 对于 20 个样本中的每一个，生成的视频必须达到 **PSNR ≥ 25**。
6. 提交的推理日志（log）文件应包含推理过程中每一步的时间消耗等关键要素。
7. 参赛者需提供模型推理过程、机器规格、环境搭建、所用优化方法及性能对比的详细细节，这将作为组委会评分的主要依据。
8. 参赛者必须按上述要求提交所有文件。**未能提供任何所需材料将导致该项评分为零。**

---