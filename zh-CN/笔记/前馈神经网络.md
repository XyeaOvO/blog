# 深度学习讲义：前馈神经网络 (Feedforward Neural Network)

---

## 📖 第一章：神经网络基础：从线性模型到非线性世界

### 1.1 线性模型的局限性 🌱 **【了解】**

在机器学习的旅程中，我们最早接触的是线性模型。它们形式简单、易于求解，是理解复杂模型的重要基石。

* **知识回顾** 💡：
    * [cite_start]**线性回归**：通过假设一个线性关系 $h_w(x) = w_0 + w_1x_1 + \dots$ 来预测连续值。其目标是最小化代价函数，如均方误差 $J(w) = \frac{1}{n}\sum(y^{(i)} - h_w(x^{(i)}))^2$ [cite: 10, 13, 17, 18]。
    * [cite_start]**Softmax 回归**：用于处理多分类问题，是线性分类模型的一种。它通过交叉熵损失函数 $\mathcal{R}(W)=-\frac{1}{N}\sum_{n=1}^{N}(y^{(n)})^{T}\log\hat{y}^{(n)}$ 进行学习 [cite: 21, 23, 24]。

[cite_start]然而，现实世界充满了复杂的非线性关系。无论是图像识别、语音处理还是自然语言理解，简单的线性函数都不足以捕捉其内在规律 [cite: 28]。

### 1.2 构造非线性：函数复合的力量 🧩 **【重点】**

[cite_start]既然单个函数能力有限，一个自然的想法就是将简单的函数复合起来，构建一个更强大的复杂函数 [cite: 34]。这正是神经网络思想的萌芽。

[cite_start]想象一下，数据（苹果）进入第一个函数 `g`，被处理成中间状态（柠檬），这个中间状态再被送入第二个函数 `f`，最终得到我们想要的结果（橘子）[cite: 39, 40][cite_start]。这个过程就是函数复合 $f \circ g$，即 $f(g(x))$ **（如图 1.1 所示）** [cite: 48]。

通过将许多简单的非线性函数层层复合，我们理论上可以构造出任意复杂的函数，从而描述复杂的世界。

[图 1.1: 函数复合的可视化解释，展示了数据流经多个函数模块的过程]

### 1.3 神经元：从生物到人工的灵感 🧠 **【重点】**

神经网络的设计灵感来源于生物大脑。

* [cite_start]**生物神经元**：是大脑的基本处理单元。它接收来自其他神经元的信号（通过树突），在细胞体内进行处理，当信号强度超过某个阈值时，神经元被“激活”（兴奋），并通过轴突将信号传递给下一个神经元 [cite: 73, 79][cite_start]。它只有“兴奋”和“抑制”两种状态 [cite: 79]。

* [cite_start]**人工神经元 (Artificial Neuron)**：是生物神经元的数学模型，也被称为**感知机 (Perceptron)**。它模拟了生物神经元的核心功能 [cite: 89]：
    1.  [cite_start]**加权求和**：接收多个输入 $x_1, x_2, \dots, x_n$，每个输入都被赋予一个权重 $w_1, w_2, \dots, w_n$。模型将所有加权的输入进行求和，并加上一个偏置项 $b$ [cite: 87, 90, 101, 106]。
    2.  [cite_start]**非线性变换 (激活)**：将加权和的结果输入到一个非线性函数——**激活函数 (Activation Function)** 中，产生最终的输出 [cite: 88, 100]。

[cite_start]其基本结构**（如图 1.2 所示）**，数学上可以表示为对输入进行加权计算 [cite: 106]。

[图 1.2: 人工神经元的基本结构，展示了输入、权重、求和、激活函数和输出]

## 🚀 第二章：感知机及其演进

### 2.1 感知机：一个简单的线性分类器 **【重点】**

[cite_start]感知机是最早的人工神经网络模型。它使用一个非常简单的激活函数——符号函数 ($sgn$)，将加权和的结果映射为两个离散的输出（例如 +1 或 -1），从而实现二分类 [cite: 113]。

> **核心思想** 🎯
>
> 感知机通过在数据空间中寻找一个**超平面**（在二维空间中就是一条直线），将不同类别的数据点分离开。
>
> $\boxed{h(x) = sgn(\sum_{i=0}^{n} w_i x_i)}$
>
> [cite_start]其中 $x_0=1$, $w_0$ 是偏置项（或阈值）[cite: 113, 153]。

[cite_start]**示例：是否去打网球？** [cite: 115]
我们可以根据三个因素来决策：1. [cite_start]天气好吗？($x_1$) 2. 有朋友陪吗？($x_2$) 3. 交通方便吗？($x_3$) [cite: 120, 123, 125][cite_start]。通过为这些因素设置不同的权重（重要性）和阈值，我们就能得到一个决策模型 [cite: 129, 130, 131]。

### 2.2 XOR 问题：感知机的瓶颈 🚧 **【核心考点】**

[cite_start]单个感知机本质上是一个线性分类器，它只能解决**线性可分**的问题。然而，对于像异或 (XOR) 这样的**线性不可分**问题，单个感知机就无能为力了 [cite: 203]。你无法用一条直线将 XOR 问题的两类点完美分开**（如图 2.1 所示）**。

[图 2.1: XOR 问题的可视化，展示了其线性不可分的特性]

[cite_start]Minsky 在 1969 年指出了这个问题，这曾导致神经网络研究进入了一段低谷期 [cite: 203]。

### 2.3 多层感知机 (MLP)：突破线性限制 ✨ **【核心考点】**

解决 XOR 问题的关键在于“升维”或组合多个线性分类器。通过将多个感知机组合成一个**多层网络**，我们就能构建出更复杂的非线性决策边界。

* **核心思想**：
    * 输入层 (Input Layer)：接收原始数据。
    * 隐藏层 (Hidden Layers)：由多个神经元组成，负责进行特征提取和非线性变换。一个 MLP 可以有多个隐藏层。
    * 输出层 (Output Layer)：产生最终的预测结果。

[cite_start]通过将简单的 AND、OR、NOT 等逻辑操作组合起来，可以实现复杂的 XOR 逻辑 [cite: 206][cite_start]。例如，一个两层的感知机网络就可以完美解决 XOR 问题**（如图 2.2 所示）** [cite: 205]。

> [cite_start]$\boxed{XOR(h_1, h_2) = OR(AND(\neg h_1, h_2), AND(h_1, \neg h_2))}$ [cite: 206]

这标志着从“感知机”到“多层感知机”（即**前馈神经网络**）的飞跃，使其具备了拟合任意复杂函数的能力。

[图 2.2: 使用多层感知机解决 XOR 问题的网络结构图]

## 💡 第三章：激活函数：网络的非线性之源

### 3.1 激活函数的作用与性质 **【重点】**

[cite_start]激活函数是神经网络中至关重要的一环，它决定了神经元的输出特性，并为整个网络引入了**非线性** [cite: 272]。如果一个多层网络没有激活函数（或使用线性激活函数），那么它本质上仍然等价于一个单层的线性模型。

[cite_start]一个好的激活函数通常应具备以下性质 [cite: 271]：
1.  **非线性**：这是最基本的要求，也是网络能够学习复杂模式的关键。
2.  [cite_start]**连续可导**：这是为了能使用基于梯度的优化方法（如梯度下降）来训练网络参数 [cite: 273, 274]。
3.  [cite_start]**计算简单**：导函数和函数本身计算要高效，以提高训练速度 [cite: 275, 276]。
4.  [cite_start]**合适的导数值域**：导函数的值不能过大或过小，否则会导致梯度爆炸或梯度消失，影响训练稳定性 [cite: 277, 278]。

### 3.2 Sigmoid 与 Tanh：经典的 "S" 型函数 **【重点】**

[cite_start]在早期，Sigmoid 函数因其平滑、可导的特性，常被用来替代感知机中的符号函数 [cite: 243, 244]。

* **Sigmoid (Logistic) 函数**
    * [cite_start]公式：$\sigma(x) = \frac{1}{1+exp(-x)}$ [cite: 281]
    * [cite_start]特点：将任意实数输入压缩到 $(0, 1)$ 区间 [cite: 247]，可以被解释为概率。
    * 缺点：
        1.  [cite_start]**梯度消失**：当输入值非常大或非常小时，其导数趋近于 0 **(如图 3.1 所示)**，导致在反向传播时梯度信号变得非常微弱，深层网络难以训练 [cite: 349, 350]。
        2.  输出非零中心，可能影响收敛速度。

* **Tanh (双曲正切) 函数**
    * [cite_start]公式：$tanh(x) = \frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$ [cite: 282]
    * 特点：将输入压缩到 $(-1, 1)$ 区间，输出是**零中心**的，通常比 Sigmoid 函数收敛更快。
    * [cite_start]缺点：仍然存在**梯度消失**问题 [cite: 382, 383]。

[图 3.1: Sigmoid 函数及其导数图像，显示了两端饱和区的梯度趋近于零]

### 3.3 ReLU 及其变体：现代网络的首选 🌟 **【核心考点】**

为了解决梯度消失问题，**修正线性单元 (Rectified Linear Unit, ReLU)** 被提出，并迅速成为现代深度学习中最受欢迎的激活函数。

* **ReLU 函数**
    * [cite_start]公式：$ReLU(x) = max(0, x)$ [cite: 298]
    * 优点：
        1.  [cite_start]**计算高效**：实现非常简单 [cite: 293]。
        2.  [cite_start]**单侧抑制**：在 $x<0$ 时输出为 0，提供了网络的稀疏性 [cite: 295]。
        3.  [cite_start]**缓解梯度消失**：在 $x>0$ 的区域，梯度恒为 1，有效缓解了梯度消失问题 [cite: 296]。
    * 缺点：
        1.  [cite_start]**Dying ReLU Problem (死亡 ReLU 问题)**：如果一个神经元的输入在训练过程中恒为负，那么它的梯度将永远是 0，导致该神经元的参数无法再被更新 [cite: 311]。

为了解决“死亡 ReLU”问题，研究者提出了一系列变体：
* [cite_start]**Leaky ReLU**：为负数区域赋予一个微小的非零斜率 $\gamma$。$f(x) = \begin{cases} x & \text{if } x>0 \\ \gamma x & \text{if } x \le 0 \end{cases}$ [cite: 299]。
* **PReLU**：将 $\gamma$ 作为一个可学习的参数。
* [cite_start]**ELU (Exponential Linear Unit)**：在负数区域使用指数函数，使得输出均值更接近 0。$f(x) = \begin{cases} x & \text{if } x>0 \\ \gamma(exp(x)-1) & \text{if } x \le 0 \end{cases}$ [cite: 299]。

[cite_start]其他常用函数还包括 **Softplus** 和 **Swish** 等 [cite: 300, 313]。**（如图 3.2 所示）**

[图 3.2: ReLU, Leaky ReLU, ELU, Softplus 函数的图像对比]

### 3.4 常用激活函数总结 **【重点】**

[cite_start]下表总结了常用激活函数的公式及其导数 [cite: 323, 324]。

| 激活函数 | 函数 | 导数 |
| :--- | :--- | :--- |
| Logistic (Sigmoid) | $f(x)=\frac{1}{1+exp(-x)}$ | $f'(x)=f(x)(1-f(x))$ |
| Tanh | $f(x)=\frac{exp(x)-exp(-x)}{exp(x)+exp(-x)}$ | $f'(x)=1-f(x)^2$ |
| ReLU | $f(x)=max(0,x)$ | $f'(x)=I(x>0)$ |
| Softplus | $f(x)=log(1+exp(x))$ | $f'(x)=\frac{1}{1+exp(-x)}$ |

## 🏗️ 第四章：前馈神经网络 (Feedforward Neural Network)

### 4.1 网络结构剖析 **【重点】**

前馈神经网络 (FNN)，有时也称为多层感知机 (MLP)，是最基本也是最常用的一种网络结构。其核心特征是：
1.  [cite_start]**分层结构**：神经元被组织在不同的层中，包括输入层、一个或多个隐藏层和输出层 [cite: 472, 480, 481, 482]。
2.  [cite_start]**单向传播**：信号从输入层开始，逐层向前传播，直到输出层，网络中不存在环路或反馈 [cite: 404, 475]。
3.  [cite_start]**全连接**：通常情况下，相邻两层之间的神经元是完全连接的，即前一层中的每个神经元都与后一层中的所有神经元相连 [cite: 473]。

[图 4.1: 一个包含输入层、两个隐藏层和一个输出层的前馈神经网络结构图]

### 4.2 前向传播的数学描述 🔢 **【核心考点】**

[cite_start]信息在网络中向前传递的过程被称为**前向传播 (Forward Propagation)**。我们可以用一系列矩阵运算来精确描述这个过程 [cite: 499]。

[cite_start]我们使用以下记号来描述一个 $L$ 层的网络 [cite: 490]：
* [cite_start]$a^{(l)}$：第 $l$ 层神经元的**激活值 (输出)**。$a^{(0)}$ 就是输入样本 $x$ [cite: 503]。
* $W^{(l)}$：从第 $l-1$ 层到第 $l$ 层的**权重矩阵**。
* $b^{(l)}$：从第 $l-1$ 层到第 $l$ 层的**偏置向量**。
* $z^{(l)}$：第 $l$ 层神经元的**加权输入 (净输入)**。
* $f_l(\cdot)$：第 $l$ 层神经元的激活函数。

前向传播的核心计算过程如下：

> **【核心公式】** 🎯
>
> 对于网络的第 $l$ 层 ($l=1, \dots, L$)：
>
> 1.  计算加权输入：
>     [cite_start]$\boxed{z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}}$ [cite: 500, 562]
> 2.  通过激活函数得到输出：
>     [cite_start]$\boxed{a^{(l)} = f_l(z^{(l)})}$ [cite: 500, 566]

[cite_start]整个网络的计算流程就是从 $a^{(0)}=x$ 开始，重复上述步骤，直到计算出最终的输出 $a^{(L)}$ [cite: 503]。

### 4.3 通用近似定理：FNN 为何如此强大？ 👑 **【重点】**

一个关键的理论问题是：一个 FNN 究竟能做什么？**通用近似定理 (Universal Approximation Theorem)** 回答了这个问题。

> **【核心定理】** 🎯
>
> [cite_start]**通用近似定理** [cite: 514, 1129]：
>
> [cite_start]对于一个具有线性输出层和至少一个使用“挤压”性质激活函数（如 Sigmoid）的隐藏层的前馈神经网络，只要其隐藏层神经元的数量足够，**它可以以任意的精度来近似任何一个定义在实数空间有界闭集上的连续函数** [cite: 521]。
>
> [cite_start]这个定理后来也被证明对其他激活函数（如 ReLU）同样成立 [cite: 1129]。

[cite_start]这个定理告诉我们，FNN 是一种非常强大的“万能函数逼近器” [cite: 525][cite_start]。只要网络足够大，它就能模拟出极其复杂的数据转换或条件分布 [cite: 526]。

### 4.4 输出层设计：适配不同任务 **【重点】**

[cite_start]输出层的设计取决于具体的机器学习任务 [cite: 569]。

* [cite_start]**回归任务**：如果目标是预测一个连续值（如房价），输出层通常只有一个神经元，并且可以使用线性激活函数（即不使用激活函数）或者根据输出的值域选择合适的函数（如 ReLU 保证输出非负）[cite: 570]。
* **二分类任务**：如果目标是二选一（如判断邮件是否为垃圾邮件），输出层通常也只有一个神经元，并使用 **Sigmoid** 激活函数，其输出可以解释为属于正类的概率。
* [cite_start]**多分类任务**：如果目标是从多个类别中选择一个（如手写数字识别），输出层神经元的数量等于类别总数，并使用 **Softmax** 函数 [cite: 571]。

> [cite_start]**Softmax 函数** [cite: 580]：
>
> [cite_start]$softmax(z_j) = \frac{e^{z_j}}{\sum_{k=1}^{K} e^{z_k}}$ [cite: 608, 609]
>
> [cite_start]Softmax 函数可以将一个任意实数的向量，转换为一个和为 1 的概率分布向量，每个分量代表了样本属于对应类别的概率 [cite: 605]。

## ⚙️ 第五章：网络训练：反向传播算法

### 5.1 学习目标：最小化损失函数 **【重点】**

[cite_start]训练一个神经网络的目的是找到一组最优的参数（权重 $W$ 和偏置 $b$），使得网络对于给定的输入 $x$，其输出 $\hat{y}$ 与真实的标签 $y$ 尽可能接近。我们通过一个**损失函数 (Loss Function)** 或代价函数来衡量这种“接近”的程度 [cite: 721]。

* [cite_start]对于回归问题，常用**均方误差 (MSE)** 损失：$J = \frac{1}{2n}\sum ||y(x) - \hat{y}(x)||^2$ [cite: 722]。
* [cite_start]对于多分类问题，常用**交叉熵 (Cross-Entropy)** 损失：$\mathcal{L}(y, \hat{y}) = - \sum y_i \log(\hat{y}_i)$ [cite: 666, 667]。

[cite_start]我们的目标就是调整 $W$ 和 $b$，来最小化在整个训练数据集上的总损失 [cite: 670, 672]。

### 5.2 优化方法：梯度下降 **【重点】**

[cite_start]**梯度下降 (Gradient Descent)** 是训练神经网络最常用的优化算法 [cite: 25, 671]。

* **知识回顾** 💡：梯度是一个向量，指向函数值上升最快的方向。因此，梯度的反方向就是函数值下降最快的方向。

[cite_start]梯度下降的更新规则非常直观 [cite: 678]：
1.  [cite_start]计算损失函数 $\mathcal{L}$ 关于每个参数（例如 $w$）的**梯度 (偏导数)** $\frac{\partial \mathcal{L}}{\partial w}$ [cite: 679]。
2.  [cite_start]沿着梯度的**反方向**更新参数，步长由**学习率** $\alpha$ 控制：$w \leftarrow w - \alpha \frac{\partial \mathcal{L}}{\partial w}$ [cite: 681]。

### 5.3 挑战：如何高效计算梯度？ 🤯 **【核心考点】**

[cite_start]神经网络是一个非常复杂的复合函数，包含成千上万甚至数百万个参数。直接使用定义去计算每个参数的梯度是极其低效的。我们需要一个高效的算法来完成这个任务，这就是**反向传播 (Backpropagation, BP)** 算法的用武之地 [cite: 686, 689]。

[cite_start]反向传播算法的核心思想是巧妙地运用**链式法则 (Chain Rule)** [cite: 687, 747]。

### 5.4 反向传播：核心思想 🧠 **【核心考点】**

反向传播算法分为两个阶段：

1.  **前向传播 (Forward Pass)**：输入一个样本 $x$，按照 4.2 节的公式，计算出网络每一层的激活值，直到得到最终的输出 $\hat{y}$，并计算出该样本的损失值。
2.  **反向传播 (Backward Pass)**：损失值会产生一个“误差信号”，这个信号从输出层开始，**逐层向后**传播。在每一层，我们利用链式法则计算出损失函数关于该层参数 ($W^{(l)}$, $b^{(l)}$) 的梯度，以及传递到前一层的误差信号。

> **核心思想** 🎯
>
> 反向传播的本质是**将最终的误差（损失）逐层分解，并归因于每一层的每一个参数**。它高效地计算了损失函数对网络中所有参数的梯度。

### 5.5 反向传播的四个核心公式 **【核心考点】**

为了实现反向传播，我们定义一个关键变量：第 $l$ 层第 $j$ 个神经元的**误差项** $\delta_j^l$，它表示损失函数对该神经元加权输入 $z_j^l$ 的偏导数。

> [cite_start]$\delta_j^l \triangleq \frac{\partial J}{\partial z_j^l}$ [cite: 822]

[cite_start]反向传播算法可以通过以下四个核心公式来概括 [cite: 851]：

1.  **(BP1) 输出层误差**：
    [cite_start]$\boxed{\delta^L = \nabla_a J \odot \sigma'(z^L)}$ [cite: 823, 852]
    * **含义**：输出层的误差等于“损失对输出的梯度”与“输出层激活函数的导数”的逐元素乘积。它直接关联了网络的最终输出与真实标签的差距。

2.  **(BP2) 误差反向传播**：
    [cite_start]$\boxed{\delta^l = ((W^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l)}$ [cite: 830, 852]
    * **含义**：第 $l$ 层的误差，可以通过第 $l+1$ 层的误差 $\delta^{l+1}$ 反向传播计算得到。这是算法“反向传播”之名的由来，也是连接各层误差的关键。

3.  **(BP3) 权重梯度**：
    [cite_start]$\boxed{\frac{\partial J}{\partial w_{jk}^l} = a_k^{l-1} \delta_j^l}$ [cite: 836, 855]
    * **含义**：损失对某个权重 $w_{jk}^l$ 的梯度，等于该权重连接的前一层神经元激活值 $a_k^{l-1}$ 与后一层神经元误差 $\delta_j^l$ 的乘积。

4.  **(BP4) 偏置梯度**：
    [cite_start]$\boxed{\frac{\partial J}{\partial b_j^l} = \delta_j^l}$ [cite: 843, 857]
    * **含义**：损失对某个偏置 $b_j^l$ 的梯度，就等于该神经元的误差项 $\delta_j^l$。

**算法流程**：
[cite_start]通过 BP1 计算出最后一层的误差 $\delta^L$，然后利用 BP2 逐层向前计算出所有隐藏层的误差 $\delta^{L-1}, \dots, \delta^2$。一旦所有层的误差项都计算完毕，就可以用 BP3 和 BP4 计算出所有参数的梯度，最后通过梯度下降来更新参数 [cite: 861, 862, 863, 864, 865, 866, 867, 868, 869]。

## 📈 第六章：现代视角：计算图与自动微分

### 6.1 计算图的概念 **【了解】**

[cite_start]反向传播可以被看作是一种更通用的技术——**自动微分 (Automatic Differentiation, AD)** 在神经网络上的一个特例 [cite: 692][cite_start]。自动微分的核心是**计算图 (Computation Graph)** [cite: 894]。

[cite_start]计算图是一种将任何数学表达式表示为有向无环图 (DAG) 的方法。图中的节点代表变量或操作，边代表了变量之间的依赖关系 [cite: 894]。

[cite_start]例如，一个简单的 Sigmoid 函数 $f(x; w, b) = \frac{1}{exp(-(wx+b))+1}$ 可以被分解为一系列基本操作，并表示为一个计算图**（如图 6.1 所示）** [cite: 895]。

[图 6.1: Sigmoid 函数的计算图表示，将复杂函数分解为一系列原子操作]

### 6.2 自动微分：前向模式与反向模式 **【重点】**

基于计算图，自动微分主要有两种模式：

* **前向模式 (Forward Mode)**：从输入到输出，计算每个节点相对于输入的导数。
* [cite_start]**反向模式 (Reverse Mode)**：从最终输出到输入，计算输出相对于每个节点的导数。这正是**反向传播算法的本质** [cite: 969]。当输出是标量（如损失函数）而输入是高维向量（如网络参数）时，反向模式的计算效率远高于前向模式。

[cite_start]现代深度学习框架（如 TensorFlow, PyTorch）都内置了自动微分引擎，使得我们只需要定义好网络的前向计算过程，框架就能自动构建计算图并执行反向传播，极大地简化了开发过程 [cite: 981, 982]。

### 6.3 静态图 vs. 动态图 **【了解】**

* [cite_start]**静态计算图 (Static Graph)**：先定义好整个计算图，然后反复执行它（“先编译，后执行”）。优点是便于优化，并行能力强，但灵活性差。代表：Theano, TensorFlow 1.x [cite: 980, 981, 983]。
* [cite_start]**动态计算图 (Dynamic Graph)**：每次运行时动态构建计算图（“即时执行”）。优点是灵活性高，调试方便，但优化和并行相对困难。代表：PyTorch, DyNet [cite: 982, 983]。

## 🛠️ 第七章：实践中的考量与应用

### 7.1 常见的优化难题 **【核心考点】**

在训练深度神经网络时，我们常常会遇到一些挑战：

1.  [cite_start]**非凸优化问题**：神经网络的损失函数通常是**非凸 (Non-Convex)** 的，存在许多局部最优解，而非全局最优解。这意味着梯度下降可能会陷入一个次优的局部最小值 [cite: 1025, 1033]。**（如图 7.1 所示）**

2.  [cite_start]**梯度消失问题 (Vanishing Gradient Problem)**：正如在 3.2 节提到的，在使用 Sigmoid 或 Tanh 等饱和激活函数时，误差在反向传播过程中经过多层后，梯度可能会变得极其微小，导致网络底层（靠近输入的层）的参数几乎无法更新，网络难以训练 [cite: 1026, 1049, 1131]。

3.  **梯度爆炸问题 (Exploding Gradient Problem)**：与梯度消失相反，在某些情况下，梯度在反向传播中会累积得非常大，导致参数更新过猛，训练过程不稳定。

[图 7.1: 神经网络损失函数的非凸曲面示意图]

### 7.2 应用实例 🌐 **【了解】**

前馈神经网络及其变体在众多领域取得了巨大成功。

* [cite_start]**图像分类**：例如，在 CIFAR-10 数据集上，我们可以构建一个多层的 FNN，输入为拉平的图像像素向量 ($3 \times 32 \times 32 = 3072$)，经过多个使用 ReLU 激活函数的隐藏层，最终通过一个 10 输出的 Softmax 层进行分类 [cite: 1081, 1096, 1109, 1110, 1111, 1112]。

* **推荐系统**：Google 的 **Wide & Deep** 模型是一个经典案例。它结合了：
    * [cite_start]**Wide 部分 (线性模型)**：负责“记忆”，直接学习历史数据中频繁共现的特征组合 [cite: 1122, 1123]。
    * [cite_start]**Deep 部分 (FNN)**：负责“泛化”，通过特征的低维嵌入 (Embeddings) 学习稀疏特征之间的复杂关联，发掘新的特征组合 [cite: 1124, 1125]。
    
    [cite_start]通过将两部分结合，模型同时具备了强大的记忆和泛化能力**（如图 7.2 所示）** [cite: 1126]。

[图 7.2: Wide & Deep 模型的结构图，展示了 Wide 和 Deep 两部分如何结合]

---
***讲义结束***